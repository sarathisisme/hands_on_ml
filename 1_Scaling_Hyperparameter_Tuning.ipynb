{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.ytimg.com/vi/b0L_2jKEbA4/maxresdefault.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will see a short example of how to scale your data and tune the hyperparamters of your models using grid or random search. \n",
    "\n",
    "We will use the well-known titanic dataset. Since you've already worked your way through the steps of exploring and cleaning the data as well as selecting proper features for modelling in another notebook, we will skip this part here and use the **preprocessed data** from the logistic regression notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# eye candy plots\n",
    "#plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-light.mplstyle')\n",
    "# source https://github.com/dhaitz/matplotlib-stylesheets\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "RSEED = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import data \n",
    "df = pd.read_csv('data/titanic_preprocessed.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test-split\n",
    "\n",
    "We will define the target and predictors and split our dataset into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define predictors and target\n",
    "y = df.Survived\n",
    "X = df.drop('Survived', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check X\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature Scaling\n",
    "\n",
    "Often the input features of your model have different units which means that the variables also have different scales. While some model types (e.g. tree-based models like decision tree or random forest) are unaffected by the scale of numerical input variables, many machine learning algorithms including for example algorithms using distance measures (e.g. KNN, SVM) perform better when the input features are scaled to a specific range. \n",
    "\n",
    "The most popular techniques for scaling are **normalization** and **standardization**. \n",
    "\n",
    "Check the [link](https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python/) for further info. \n",
    "\n",
    "![](images/normalization_vs_standardization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Before we have a look at the different methods, \n",
    "# we have to define which columns we want to scale.\n",
    "col_scale = ['Age', 'SibSp', 'Parch', 'Fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Standardization \n",
    "\n",
    "In order to standardize a dataset it is necessary to rescale the distribution of values so that the mean of observed values is 0 and the standard deviation is 1. You can think of it as subtracting the mean value or centring the data. \n",
    "Sklearn provides us for this case with the [Standard scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "\n",
    "A value is standardized as follows: \n",
    "\n",
    "$ x_{scaled} = \\frac{x – \\mu}{\\sigma}  $, where \n",
    "\n",
    "$ \\mu = \\frac{\\sum{x}}{m} $ is the mean, where m is the number of observations\n",
    "\n",
    "$ \\sigma = \\sqrt{ \\frac{\\sum{ (x – \\mu)^2 }}{m}} $ is the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transformation with column transformer\n",
    "scaling_transformation = ColumnTransformer([\n",
    "    ('scaling', StandardScaler(), col_scale)],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data normalization \n",
    "\n",
    "Normalizing the data means to rescale it from the original range so that all values lie within the new range of 0 and 1.\n",
    "We can easily do this by using the [Min-Max-Scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) from sklearn. This scaler transforms the feature(s) by scaling it(them) to a given range (default range is 0 to 1). \n",
    "\n",
    "A value is normalized as follows: \n",
    "\n",
    "$ x_{scaled} = \\frac{x – x_{min}}{x_{max} – x_{min}} $\n",
    "\n",
    "(Where the min and max values pertain to the value x being normalized, from your **train** dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ex1. Practice round\n",
    "Try out doing normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scaling with MinMaxScaler\n",
    "\n",
    "# Try to scale you data with the MinMaxScaler() from sklearn. \n",
    "# It follows the same syntax as the StandardScaler.\n",
    "# Don't forget: you have to import the scaler at the top of your notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bonus Question: Do the test standardized / normalized values follow the same properties of the train standardized / normalized values?\n",
    "\n",
    "i.e standardized: centered on 0 with stdev of 1 / normalized w values between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Predictive Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate our model performance with a quick and more reliable way using sklearn's [cross_val_score()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) which implements K-fold cross validation. When training a model based on train and test split we only have one experiment. Can we really trust one experiment? \n",
    "\n",
    "Think of [K-fold cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) as doing K experiments and then taking the average error. It is still not perfect but better than 1 experiment which can randomly turn out to be really good. \n",
    "\n",
    "Whenever we have K, comes the question about the value of K.. common values are between 5 and 10 and you need to take into account the technical limitations: dataset size, compute power and available memory and time. CV takes time on large datasets.\n",
    "\n",
    "\n",
    "![](images/cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNNClassifier - unscaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit and evaluate model without hyperparameter tuning using cross validation and unscaled data \n",
    "knn_classifier = KNeighborsClassifier()\n",
    "scores = cross_val_score(knn_classifier, X_train, y_train, cv=5, n_jobs=-1)\n",
    "\n",
    "# Evaluation \n",
    "print('Score (unscaled):', round(scores.mean(), 2))\n",
    "print('Standard Deviation (unscaled):', round(scores.std(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plotting the scores and average score\n",
    "plt.axhline(y=scores.mean(), color='y', linestyle='-')\n",
    "sns.barplot(x=[1,2,3,4, 5],y=scores).set_title('Scores of the K-Folds Models - unscaled data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNNClassifier - scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit and evaluate model without hyperparameter tuning using cross validation and scaled data \n",
    "knn_classifier_scaled = Pipeline([\n",
    "    ('scaling_trans', scaling_transformation ),\n",
    "    ('knn',KNeighborsClassifier())\n",
    "])\n",
    "scores_scaled = cross_val_score(knn_classifier_scaled, X_train, y_train, cv=StratifiedKFold(5), n_jobs=-1)\n",
    "\n",
    "# Evaluation\n",
    "print('Score (scaled):', round(scores_scaled.mean(), 2))\n",
    "print('Standard Deviation (unscaled):', round(scores.std(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.axhline(y=scores_scaled.mean(), color='y', linestyle='-')\n",
    "sns.barplot(x=[1,2,3,4, 5],y=scores_scaled).set_title('Scores of the K-Folds Models - standardized data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the overall error has gone down significantly (in my case an increase of 13% in the evaulation score), but the errors of the folds are rather similar to each. To show the effect of scaling a bit clearer we will look at another classifier that is even more affected by it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier - unscaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD classifier stands for stochastic gradient descent classifier and implements a variety of classifiers - https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM). We will use it here to further emphasise the importance of scaling and will set the loss parameter to `log_loss`. This way the model will be a logistic regression using stochastic gradient descent for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit and evaluate model without hyperparameter tuning using cross validation and unscaled data \n",
    "sgd_classifier = SGDClassifier(random_state=RSEED, loss ='log_loss')\n",
    "scores = cross_val_score(sgd_classifier, X_train, y_train, cv=StratifiedKFold(5), n_jobs=-1)\n",
    "\n",
    "# Evaluation \n",
    "print('Score (unscaled):', round(scores.mean(), 2))\n",
    "print('Standard Deviation (unscaled):', round(scores.std(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plotting the scores and average score\n",
    "plt.axhline(y=scores.mean(), color='y', linestyle='-')\n",
    "sns.barplot(x=[1,2,3,4, 5],y=scores).set_title('Scores of the K-Folds Models - unscaled data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation error varies a lot for the different folds of the data (from around 0.5 to a high of almost 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit and evaluate model without hyperparameter tuning using cross validation and scaled data \n",
    "sgd_classifier_scaled = Pipeline([\n",
    "    ('scaling_trans', scaling_transformation ),\n",
    "    ('sgd', SGDClassifier(random_state=RSEED,loss ='log_loss'))\n",
    "\n",
    "])\n",
    "\n",
    "scores_scaled = cross_val_score(sgd_classifier_scaled, X_train, y_train, cv=StratifiedKFold(5), n_jobs=-1)\n",
    "\n",
    "# Evaluation\n",
    "print('Score (scaled):', round(scores_scaled.mean(), 2))\n",
    "print('Standard Deviation (unscaled):', round(scores.std(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.axhline(y=scores_scaled.mean(), color='y', linestyle='-')\n",
    "sns.barplot(x=[1,2,3,4, 5],y=scores_scaled).set_title('Scores of the SGD Models - standardized data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beside the obviously better overall performance an important effect of scaling visible here is that the validation error doesn't vary quite as much as it used with most of the values being closer to the 0.8 mark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ex2. Practice round\n",
    "Try out KNNClassifier with the normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using the MinMaxScaler data you scaled in ex1.\n",
    "\n",
    "# It follows the same syntax as the StandardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most models have many hyperparameters that work better with some datasets than with others. Same goes with the hyperparameters from regularization which we learned that are selected based on a trial and error process. So how do we manage to select the parameter values that work best for our data?\n",
    "\n",
    "#### GridSearchCV\n",
    "\n",
    "[Grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) is a tuning technique that attempts to compute the optimum values of hyperparameters. It performs an exhaustive search over a prior defined parameter space using cross-validation (hence the **CV** suffix). That means it will evaluate all of the possible parameter combinations of the search space in order to find and return the best combination. \n",
    "\n",
    "\n",
    "This task, however, starts to become very time-consuming if there are many hyperparameters and the search space is huge. As you can see for k= 5 and for 2 parameters with 2, and respectively 3 values, thus 6 combinations, the GridSearchCV runs 30 modelling steps in order to just come up with the best values for the two parameters.\n",
    "\n",
    "![](images/grid_search_cv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#what hyperparameters does KNN have?\n",
    "knn_classifier_scaled.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining parameter grid (as dictionary)\n",
    "param_grid = {\"knn__n_neighbors\" : [2,4,3,5,10], #this actually defines the model you use\n",
    "              \"knn__weights\" : [\"uniform\", \"distance\"],\n",
    "              \"knn__p\" : [1, 2, 3],\n",
    "             }\n",
    "\n",
    "# Instantiate gridsearch and define the metric to optimize \n",
    "gs = GridSearchCV(knn_classifier_scaled, param_grid, scoring='accuracy',\n",
    "                  cv=5, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit gridsearch object to data.. also lets see how long it takes\n",
    "start = timer()\n",
    "gs.fit(X_train, y_train)\n",
    "end = timer()\n",
    "gs_time = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gs_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best score\n",
    "print('Best score:', round(gs.best_score_, 3))\n",
    "\n",
    "# Best parameters\n",
    "print('Best parameters:', gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we will do this at least twice.. according to DRY we should write a function\n",
    "def print_pretty_summary(name, best_model, y_test, y_pred_test):\n",
    "    print(name)\n",
    "    print('=======================')\n",
    "    print('n_neighbors: {}'.format(best_model.steps[-1][1].n_neighbors))\n",
    "    print('weights: {}'.format(best_model.steps[-1][1].weights))\n",
    "    print('p: {}'.format(best_model.steps[-1][1].p))\n",
    "    print('algorithm: {}'.format(best_model.steps[-1][1].algorithm))\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    print('Test accuracy: {:2f}'.format(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_best = gs.best_estimator_\n",
    "knn_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assigning the fitted KNNClassifier model with best parameter combination to a new variable knn_best\n",
    "knn_best = gs.best_estimator_\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_test = knn_best.predict(X_test)\n",
    "# Let us print out the performance of our model on the test set.\n",
    "knn_accuracy = print_pretty_summary('KNNClassifier model', knn_best, y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "\n",
    "As an alternative to grid search we can use sklearn's RandomizedSearchCV(). Random search will not try every possible combination of our search space but will randomly pick and evaluate parameter combinations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define parameter grid for randomized search\n",
    "param_grid = {\"knn__n_neighbors\" : [2,4,3,5,10], #this actually defines the model you use\n",
    "              \"knn__weights\" : [\"uniform\", \"distance\"],\n",
    "              \"knn__p\" : [1, 2, 3],\n",
    "             }\n",
    "# Instantiate random search and define the metric to optimize \n",
    "rs = RandomizedSearchCV(knn_classifier_scaled, param_grid, scoring='accuracy',\n",
    "                  cv=5, verbose=5, n_jobs=-1, random_state=RSEED)\n",
    "\n",
    "# Fit randomized search object to data\n",
    "start = timer()\n",
    "rs.fit(X_train, y_train)\n",
    "end = timer()\n",
    "rgs_time = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best score\n",
    "print('Best score:', round(rs.best_score_, 3))\n",
    "\n",
    "# Best parameters\n",
    "print('Best parameters:', rs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assigning the fitted KNNClassifier model with best parameter combination to a new variable knn_best\n",
    "knn_best_rs = rs.best_estimator_\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_test_rs = knn_best_rs.predict(X_test)\n",
    "\n",
    "\n",
    "# Let us print out the performance of our model on the test set.\n",
    "rknn_accuracy = print_pretty_summary('KNNClassifier model (randomizedGSCV)', knn_best_rs, y_test, y_pred_test_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Grid search took {gs_time} seconds to run with accuracy: {knn_accuracy:f}\")\n",
    "print(f\"Randomized Grid search took {rgs_time} seconds to run with accuracy: {rknn_accuracy:f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an educational example... more data means longer train times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
