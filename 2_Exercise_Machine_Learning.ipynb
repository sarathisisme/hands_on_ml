{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44e7c80e",
   "metadata": {},
   "source": [
    "# Machine Learning Exercise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d646bf4d",
   "metadata": {},
   "source": [
    "Now it's time to practice what you have seen in the previous notebooks. Your task for today is to download the data from the database and train a model in order to predict if a patient has a heart disease or not. \n",
    "\n",
    "![](https://www.nicepng.com/png/detail/397-3975460_disease-high-quality-png-heart-disease-cartoon-png.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3eaca75",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Task:\n",
    "\n",
    "1. Import the data from the database. The schema is called `heart`. You can use DBeaver to get an overview over the different tables and think about a good way to join them. \n",
    "2. Conduct a brief EDA to become familiar with the data. \n",
    "3. Preprocess the data as far as you need it and...\n",
    "4. ...train a logistic regression model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cef7fa38",
   "metadata": {},
   "source": [
    "## What you should use/keep in mind:\n",
    " \n",
    "* **Scale your data:** Which scaler works best in your case?\n",
    "* **Tune your model:** Tune the hyperparameter of your model. You can start with a larger parameter grid and a `RandomizedSearchCV` and continue with a narrower parameter grid for your `GridSearchCV`.\n",
    "* **Choose the right evaluation metric!**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4f69f0c",
   "metadata": {},
   "source": [
    "## Data Overview\n",
    "\n",
    "| column | additional information |\n",
    "|--------|------------------------|\n",
    "| age | age of patient |\n",
    "| sex | gender of patient |\n",
    "| chest_pain_type  | 1 = typical angina, 2 = atypical angina, 3 = non-anginal pain, 4 = asymptomatic | \n",
    "| resting_blood_pressure |  | \n",
    "| fasting_blood_sugar | > 120 mg/dl, 1 = true, 0 = false | \n",
    "| thal | 0 = normal, 1 = fixed defect, 2 = reversable defect\n",
    "| serum_cholestoral | in mg/dl | \n",
    "| resting_electrocardiographic_results | 0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria | \n",
    "| maximum_heartrate_achieved | | \n",
    "| exercise_induced_angina | 1 = yes, 0 = no | \n",
    "| oldpeak | ST depression induced by exercise relative to rest | \n",
    "| slope_of_the_peak_exercise_st_segment | 1= upsloping, 2 = flat, 3 = downsloping | \n",
    "| number_of_major_vessels_colored_by_flourosopy | |\n",
    "| real_data | tag to distinguish between real and made up data | \n",
    "| heart_attack | 0 = little risk of heart attack, 1 = high risk of heart attack | "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b139eac-a14c-4f50-8c80-36a801f00ca7",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f34948-8795-4c27-867f-c98478949e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import skew\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "# Modelling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import plotly.express as px\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "plt.rcParams.update({ \"figure.figsize\" : (8, 5),\"axes.facecolor\" : \"white\", \"axes.edgecolor\":  \"black\"})\n",
    "plt.rcParams[\"figure.facecolor\"]= \"w\"\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RSEED = 42\n",
    "\n",
    "\n",
    "# Feel free to add all the libraries you need"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29326ef9-f51c-4531-9337-88dbdf4c93b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting the Data\n",
    "\n",
    "The data for this exercise is stored in our postgres database in the schema `heart`. The different features are split thematically into five different tables. Your first task will be to have a look at the tables (e.g. in DBeaver) and figure out a way to join the information you need. As soon as you're happy with your query, you can use the following code cells to import the data into this notebook. \n",
    "\n",
    "In previous notebooks you've seen two different approaches to import data from a database into a notebook. The following code will use `sqlalchemy`in combination with pandas `pd.read_sql()` function. For the code to work, you need to copy the `.env` file from the previous repositories into this repository and change the query_string to your own query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116f65b-5c98-4e8d-bc88-6239bd401168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read database string from .env file (no need to change anything)\n",
    "load_dotenv()\n",
    "\n",
    "DB_STRING = os.getenv('DB_STRING')\n",
    "\n",
    "db = create_engine(DB_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b1d420-3c92-4530-92a2-3c215c0c9bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define query to download data (add your query here)\n",
    "query_string = ''' \n",
    "set SCHEMA 'heart';\n",
    "SELECT p.id as patient_id,p.age,p.sex, \n",
    "cp.chest_pain_type,\n",
    "bm.serum_cholestoral, bm.fasting_blood_sugar,bm.thal, \n",
    "pva.resting_blood_pressure,pva.resting_electrocardiographic_results,pva.maximum_heartrate_achieved,pva.exercise_induced_angina,pva.oldpeak,pva.slope_of_the_peak_exercise_st_segment,\n",
    "pva.number_of_major_vessels_colored_by_flourosopy,pva.real_data,\n",
    "hah.heart_attack\n",
    "FROM patient p\n",
    "LEFT JOIN chest_pain cp\n",
    "ON p.id = cp.patient_id\n",
    "LEFT JOIN blood_metrics bm\n",
    "ON p.id = bm.patient_id\n",
    "LEFT JOIN pressure_vessels_angina pva\n",
    "ON p.id = pva.patient_id\n",
    "LEFT JOIN heart_attack_history hah\n",
    "ON p.id = hah.patient_id\n",
    "WHERE pva.real_data <> 'Evgeny likes white wine for lunch and red wine for dinner' or pva.real_data is NULL\n",
    "ORDER BY p.id ASC;\n",
    "'''\n",
    "\n",
    "# Import with pandas\n",
    "df_sqlalchemy = pd.read_sql(query_string, db)\n",
    "df_sqlalchemy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c132d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e9ad0-611f-4452-ac54-fb0d7d4710b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe as .csv file\n",
    "df_sqlalchemy.to_csv(\"data/heart_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb137372",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/heart_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff1d89-70f4-4e84-8c56-28c48cbb44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5682a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the shape of the data\n",
    "print('Heart dataset')\n",
    "print('==================')\n",
    "print('# observations: {}'.format(df.shape[0]))\n",
    "print('# features:     {}'.format(df.shape[1]-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7478469",
   "metadata": {},
   "source": [
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb0a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862382df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for type of data present in the data frame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508bf9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the descriptive statistics\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca120736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for unique values\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2ea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for null values in each column\n",
    "df.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db8a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicates\n",
    "\n",
    "def check_duplicates(data):\n",
    "    has_dup = data.duplicated()\n",
    "    true_dup = np.where(has_dup == True)\n",
    "    if len(true_dup[0]) > 0:\n",
    "        print(\"Data has\", len(true_dup[0]), \"duplicates\")\n",
    "    else:\n",
    "        print(\"No duplicates found !!!\")\n",
    "\n",
    "check_duplicates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285db0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for data imbalance\n",
    "df['heart_attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d531ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of features \n",
    "features = df.columns.tolist()\n",
    "features.remove('heart_attack')\n",
    "\n",
    "fig,ax = plt.subplots(8,3,figsize=(34,30))\n",
    "count = 0\n",
    "for item in features:\n",
    "    sns.histplot(df[item], kde=True, ax=ax[int(count/3)][count%3], color='#33658A').set(title=item, xlabel='')\n",
    "    count += 1\n",
    "ax.flat[-1].set_visible(False)\n",
    "fig.tight_layout(pad=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98116c55",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column names\n",
    "\n",
    "df.rename(columns = {'fasting_blood_sugar' : 'fasting_bs',\n",
    "          'resting_blood_pressure' : 'resting__bp',\n",
    "          'resting_electrocardiographic_results' : 'electrocardiography',\n",
    "          'maximum_heartrate_achieved' : 'max_heartrate',\n",
    "          'exercise_induced_angina' : 'exercise_ind_angina',\n",
    "          'slope_of_the_peak_exercise_st_segment' : 'slope_of_peak_exercise',\n",
    "          'number_of_major_vessels_colored_by_flourosopy' : 'major_vessels_colored'},\n",
    "          inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e3cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for Outliers without heart attack column\n",
    "df_outliers = df[['age', 'sex', 'chest_pain_type', 'serum_cholestoral', 'fasting_bs', 'thal', 'resting__bp', 'electrocardiography', 'max_heartrate',\n",
    "       'exercise_ind_angina', 'oldpeak', 'slope_of_peak_exercise',\n",
    "       'major_vessels_colored', 'real_data']].copy()\n",
    "\n",
    "df_outliers.plot(kind='box', subplots=True, layout=(8,3), figsize=(34,30))\n",
    "plt.show() #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e8193f9",
   "metadata": {},
   "source": [
    "## Analysing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee2fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the target variable\n",
    "plt.title('heart Attack')\n",
    "sns.countplot(x=df.heart_attack);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "775a2016",
   "metadata": {},
   "source": [
    "The dataset is well balanced - not deviating by much from a 50%-%50 target variable split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c21fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.real_data = np.where(df.real_data == 'real data', 'real data','null')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "129167dc",
   "metadata": {},
   "source": [
    "It is not clear whether NULL values in the column real_data is legitimate data or not. In order to check this we could \n",
    "compare the distributions of the other variables as a function of real_data.  \n",
    "Check the distributions appearing on the diagonal of the below plot. Do those look similar or different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87b3648",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue=\"real_data\", height=2,);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "618a03b2",
   "metadata": {},
   "source": [
    "The distributions look very similar - they are overlapping, peaking at the same values. Formally we could test whether those distributions are identical, but for now visual inspection is enough to accept NULL values as legitimate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33b6ecbe",
   "metadata": {},
   "source": [
    "Now let's inspect how the variables differ depending on our target variable - 'heart attack'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bab29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue=\"heart_attack\", height=2);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11f8de0b",
   "metadata": {},
   "source": [
    "There are some significant differences now - some of the variables with big differences seem to be chest_pain_type, thal, resting_electrocardiographic_results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24413e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# Create a new DataFrame that only includes the numerical variables\n",
    "df_numeric = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Compute correlations\n",
    "correlations = df_numeric.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(correlations)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(140, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "\n",
    "sns.heatmap(correlations, mask=mask, cmap=cmap, vmax=1, annot=True,\n",
    "            linewidths=1, cbar_kws={\"shrink\": .7}, ax=ax);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94cdffd4",
   "metadata": {},
   "source": [
    "#### Preprocessing of the Data before Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e3b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have a very large number of NaN values. \n",
    "df.dropna().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d00db",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_mask = df.drop(columns='real_data').isnull().any(axis=1)\n",
    "\n",
    "# Filter out data using a negative mask and assign it a new, separated df\n",
    "df = df[~drop_mask].copy()\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48cf20e2",
   "metadata": {},
   "source": [
    "Columns chest_pain_type, thal, resting_electrocardiographic_results and slope_of_the_peak_exercise_st_segment are currently encoded as numeric, but looking at the plot above it is clear that they represent categorical information rather than numeric. Let's convert them to categories. In the absence of domain expertise we will continue treating number_of_major_vessels_colored_by_fluoroscopy as numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ffac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.real_data = df.real_data.replace('null', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2cf2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e2b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.chest_pain_type = df.chest_pain_type.astype('category')\n",
    "df.thal = df.thal.astype('category')\n",
    "df.electrocardiography = df.electrocardiography.astype('category')\n",
    "df.slope_of_peak_exercise = df.slope_of_peak_exercise.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa842bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing categorical values\n",
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d029dbf",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb015c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y\n",
    "features = df.columns.tolist()\n",
    "features.remove('patient_id')\n",
    "features.remove('heart_attack')\n",
    "\n",
    "X = df[features]\n",
    "y = df.heart_attack\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Check the shape of the data sets\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93829e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y\n",
    "features_with_id = df.columns.tolist()\n",
    "features_with_id.remove('heart_attack')\n",
    "\n",
    "X = df[features_with_id]\n",
    "y = df.heart_attack\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train_with_id, X_test_with_id, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Check the shape of the data sets\n",
    "print(\"X_train_with_id:\", X_train_with_id.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test_with_id:\", X_test_with_id.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e78a382",
   "metadata": {},
   "source": [
    "### Predictive Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63838a88",
   "metadata": {},
   "source": [
    "### Logistic regression using non-scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cb8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression without the id field\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = log_reg.predict(X_train)\n",
    "y_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656d50a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy of our model\n",
    "print(\"Accuracy on train set:\", round(accuracy_score(y_train, y_pred_train), 2))\n",
    "print(\"Accuracy on test set:\", round(accuracy_score(y_test, y_pred), 2))\n",
    "print(\"--------\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb46e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with the id field\n",
    "log_reg_with_id = LogisticRegression(max_iter=1000)\n",
    "log_reg_with_id.fit(X_train_with_id, y_train)\n",
    "\n",
    "y_pred_train_with_id = log_reg_with_id.predict(X_train_with_id)\n",
    "y_pred_with_id = log_reg_with_id.predict(X_test_with_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63072488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy of our model\n",
    "print(\"Accuracy on train set:\", round(accuracy_score(y_train, y_pred_train_with_id), 2))\n",
    "print(\"Accuracy on test set:\", round(accuracy_score(y_test, y_pred_with_id), 2))\n",
    "print(\"--------\"*10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74cf6a9d",
   "metadata": {},
   "source": [
    "Including the patient_id increases the performance of the model to 1! This makes very little sense, let's try to understand what is happening here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0102b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=df, x='patient_id', hue='heart_attack', fill=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f661a803",
   "metadata": {},
   "source": [
    "Seems like the distributions of heart attacks are not overlapping much on the id scale. That is suspicious. Let's check it a bit closer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af43cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=sns.scatterplot(data = df,x='patient_id',y = 'heart_attack')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335aa22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mimimum id with heart_attack = 0\n",
    "df.query('heart_attack == 1').patient_id.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2da684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mimimum id with heart_attack = 0\n",
    "df.query('heart_attack == 0').patient_id.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mimimum id with heart_attack = 0\n",
    "df.query('heart_attack == 1 and patient_id > 165').patient_id.min()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7119ddff",
   "metadata": {},
   "source": [
    "It looks like patients with ids from o to 165 all had heart attacks. From id 165 to 303 no one had a heart attack and for ids higher than 303 patient ids are no longer indicative of whether the patient had a heart attack or not. The person inputting the data into the database must have done this for some reason, but is there a reason to believe that the ids will follow the same pattern in the future?\n",
    "\n",
    "No there isn't any reason to believe that - most probably after patient id 303 whether the patient had a heart attack or not will have nothing to do with the patient's id. \n",
    "\n",
    "Column id in our case provides an example of a phenomena called 'leakage' - \"use of information in the model training process which would not be expected to be available at prediction time, causing the predictive scores (metrics) to overestimate the model's utility when run in a production environment\".\n",
    "\n",
    "Leakage can be difficult to detect as it affects both the training and the test set (as they are usually drawn from the same data set). \n",
    "\n",
    "https://en.wikipedia.org/wiki/Leakage_(machine_learning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d351dc34",
   "metadata": {},
   "source": [
    "### Logistic regression using scaled data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3de87347",
   "metadata": {},
   "source": [
    "#### Scaling with Standard scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f013c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_scale = ['age', 'sex', 'serum_cholestoral', 'fasting_bs',\n",
    "       'resting__bp', 'max_heartrate',\n",
    "       'exercise_ind_angina', 'oldpeak',\n",
    "       'major_vessels_colored']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[col_scale])\n",
    "X_test_scaled = scaler.transform(X_test[col_scale])\n",
    "\n",
    "print(X_train_scaled.shape)\n",
    "print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f27ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating scaled and dummy columns \n",
    "X_train_preprocessed = np.concatenate([X_train_scaled, X_train.drop(col_scale, axis=1)], axis=1)\n",
    "X_test_preprocessed = np.concatenate([X_test_scaled, X_test.drop(col_scale, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1bbcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "y_pred_train = log_reg.predict(X_train_preprocessed)\n",
    "y_pred = log_reg.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16f6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy of our model\n",
    "print(\"Accuracy on train set:\", round(accuracy_score(y_train, y_pred_train), 2))\n",
    "print(\"Accuracy on test set:\", round(accuracy_score(y_test, y_pred), 2))\n",
    "print(\"--------\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b474e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, cmap=\"YlGnBu\", annot=True, fmt='d');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6efbbf38",
   "metadata": {},
   "source": [
    "### Scaling with MinMaxScaler (Data Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd8afdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define min max scaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = minmax_scaler.fit_transform(X_train[col_scale])\n",
    "X_test_scaled = minmax_scaler.transform(X_test[col_scale])\n",
    "\n",
    "print(X_train_scaled.shape)\n",
    "print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920dc0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating scaled and dummy columns \n",
    "X_train_preprocessed2 = np.concatenate([X_train_scaled, X_train.drop(col_scale, axis=1)], axis=1)\n",
    "X_test_preprocessed2 = np.concatenate([X_test_scaled, X_test.drop(col_scale, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eca090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_preprocessed2, y_train)\n",
    "\n",
    "y_pred_train_norm = log_reg.predict(X_train_preprocessed2)\n",
    "y_pred_norm = log_reg.predict(X_test_preprocessed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b089392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print accuracy of our model\n",
    "print(\"Accuracy on train set:\", round(accuracy_score(y_train, y_pred_train_norm), 2))\n",
    "print(\"Accuracy on test set:\", round(accuracy_score(y_test, y_pred_norm), 2))\n",
    "print(\"--------\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d915ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(y_test, y_pred_norm)\n",
    "# sns.heatmap(cm, cmap=\"YlGnBu\", annot=True, fmt='d');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8a269c0",
   "metadata": {},
   "source": [
    "### Logistic Regression with Randomsearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a63b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what parameters does logistic regression has?\n",
    "logistic = LogisticRegression()\n",
    "logistic.get_params().keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0041c46b",
   "metadata": {},
   "source": [
    " 'n_jobs' is a parameter in RandomizedSearchCV and GridSearchCV which controls the number of parallel jobs to run during the search process. Here we use n_jobs=1 which means the search will be performed using a single job, running sequentially. If we use n_jobs = -1, it allows the search to utilize multiple CPU cores or processors, enabling parallel execution of the search process. This can speed up the search when working with large datasets or complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca04ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# define evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "\n",
    "# define search space\n",
    "param_grid = { \"solver\" : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "               \"penalty\" : ['none', 'l1', 'l2', 'elasticnet'],\n",
    "               \"C\" : loguniform(1e-5, 100)}\n",
    "\n",
    "\n",
    "# define Random search\n",
    "Random_search = RandomizedSearchCV(log_reg, param_grid, n_iter=500, scoring='accuracy', n_jobs=1, cv=cv, random_state=1)\n",
    "\n",
    "\n",
    "# execute Random search\n",
    "Random_search.fit(X_train_preprocessed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f311fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_RS = Random_search.predict(X_train_preprocessed)\n",
    "y_pred_RS = Random_search.predict(X_test_preprocessed)\n",
    "\n",
    "print(\"Tuned hpyerparameters :(best parameters) \",Random_search.best_params_)\n",
    "print(\"Accuracy on test set:\", round(accuracy_score(y_test, y_pred_RS), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6054a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cm = confusion_matrix(y_test, y_pred_RS)\n",
    "#sns.heatmap(cm, cmap=\"YlGnBu\", annot=True, fmt='d');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd18a8a8",
   "metadata": {},
   "source": [
    "### Logistic regression with GridSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b49717b8",
   "metadata": {},
   "source": [
    "https://medium.com/@jackstalfort/hyperparameter-tuning-using-grid-search-and-random-search-f8750a464b35"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "678bc91c",
   "metadata": {},
   "source": [
    "we need to generate the different hyperparameter values. For C we can use np.logspace, which takes the endpoints of a range, generates num numbers evenly spaced in that range, and then takes another number (10 is the default) and raises it to each number it generated.\n",
    "To generate the different types of penalties we just make a list with them\n",
    "For the solvers, since we are using both penalties, we can only use liblinear and saga. These are also in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df12c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.logspace(0, 4, num=10)\n",
    "penalty = ['l1', 'l2']\n",
    "solver = ['liblinear', 'saga']\n",
    "\n",
    "hyperparameters = dict(C=C, penalty=penalty, solver=solver)\n",
    "\n",
    "gridsearch = GridSearchCV(log_reg, hyperparameters, scoring='accuracy',\n",
    "                  cv=5, verbose=5, n_jobs=1)\n",
    "gridsearch.fit(X_train_preprocessed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0727ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_GS = gridsearch.predict(X_train_preprocessed)\n",
    "y_pred_GS = gridsearch.predict(X_test_preprocessed)\n",
    "\n",
    "print(\"Tuned hpyerparameters :(best parameters) \",gridsearch.best_params_)\n",
    "print(\"Accuracy on test set:\", round(accuracy_score(y_test, y_pred_GS), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e46bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(y_test, y_pred_GS)\n",
    "# sns.heatmap(cm, cmap=\"YlGnBu\", annot=True, fmt='d');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
